[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Project 2",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "projects/project1/hw1_blog.html",
    "href": "projects/project1/hw1_blog.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn this study, the researchers mailed out approximately 50,000 fundraising letters to past donors of a nonprofit organization, randomly assigning each individual to one of three experimental treatments. The treatments varied in key design features that aimed to lower the effective ‚Äúprice‚Äù of donating. In the control group, recipients received a standard fundraising letter that followed the organization‚Äôs typical solicitations‚Äîwith no mention of matching or challenge grants. For the treatment groups, two alternative letter formats were used: one that incorporated a matching grant offer and another that featured a challenge grant offer. In the matching treatment, an additional paragraph was inserted in the letter to announce that a prominent leadership donor would match every donation dollar‚Äîfor example, a 1:1 match, meaning that for each dollar the recipient donated, the organization would receive an extra dollar from the donor. The challenge treatment was designed to highlight the effectiveness and urgency of the fundraising effort, sometimes by using higher matching ratios such as 2:1 or 3:1.\nBeyond just the matching ratio, the experiment also varied other elements of the solicitation letter. For instance, the maximum amount available to be matched (e.g., $25,000, $50,000, or $100,000) was randomly assigned, and the letter included different suggested donation amounts based on the donor‚Äôs past giving history. This multifaceted randomization allowed the authors to disentangle whether and how explicitly lowering the ‚Äúprice‚Äù of a donation‚Äîwith the promise of additional matching funds‚Äîincreases both the likelihood that a donor will contribute and the amount they give.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_blog.html#introduction",
    "href": "projects/project1/hw1_blog.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn this study, the researchers mailed out approximately 50,000 fundraising letters to past donors of a nonprofit organization, randomly assigning each individual to one of three experimental treatments. The treatments varied in key design features that aimed to lower the effective ‚Äúprice‚Äù of donating. In the control group, recipients received a standard fundraising letter that followed the organization‚Äôs typical solicitations‚Äîwith no mention of matching or challenge grants. For the treatment groups, two alternative letter formats were used: one that incorporated a matching grant offer and another that featured a challenge grant offer. In the matching treatment, an additional paragraph was inserted in the letter to announce that a prominent leadership donor would match every donation dollar‚Äîfor example, a 1:1 match, meaning that for each dollar the recipient donated, the organization would receive an extra dollar from the donor. The challenge treatment was designed to highlight the effectiveness and urgency of the fundraising effort, sometimes by using higher matching ratios such as 2:1 or 3:1.\nBeyond just the matching ratio, the experiment also varied other elements of the solicitation letter. For instance, the maximum amount available to be matched (e.g., $25,000, $50,000, or $100,000) was randomly assigned, and the letter included different suggested donation amounts based on the donor‚Äôs past giving history. This multifaceted randomization allowed the authors to disentangle whether and how explicitly lowering the ‚Äúprice‚Äù of a donation‚Äîwith the promise of additional matching funds‚Äîincreases both the likelihood that a donor will contribute and the amount they give.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_blog.html#data",
    "href": "projects/project1/hw1_blog.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe begin by reading the Stata data file (karlan_list_2007.dta) into Python using Pandas. This dataset comes from Karlan and List‚Äôs 2007 field experiment and contains 50,000 observations (one for each fundraising letter) and over 50 variables that document experimental assignments, donation outcomes, and donor characteristics. The experimental design includes indicators for whether the letter was a control or a treatment (matching/challenge) along with features such as the match ratio (e.g., 1:1, 2:1, or 3:1) and the match threshold (e.g., $25,000, $50,000, $100,000, or unstated). In addition, the dataset contains variables that measure donor characteristics (e.g.¬†previous donation history, demographic information, and proxy measures for political environment) that allow for an analysis of heterogeneous effects.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the Stata file\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndata.head()\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows √ó 51 columns\n\n\n\n\n\nCode\ndata.describe()\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows √ó 48 columns\n\n\n\n\n\nCode\n# Group by treatment and compute average donation and response rate\ngroup_stats = data.groupby(\"treatment\").agg(\n    response_rate=(\"gave\", \"mean\"),\n    avg_donation=(\"gave\", lambda x: data.loc[x.index, \"amount\"].mean())\n)\ngroup_stats.reset_index()\n\n\n\n\n\n\n\n\n\ntreatment\nresponse_rate\navg_donation\n\n\n\n\n0\n0\n0.017858\n0.813268\n\n\n1\n1\n0.022039\n0.966873\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nThe dataset contains a mixture of experimental design variables (e.g., treatment, control, ratio, size, and various versions of the suggested donation variables) and donor characteristics (e.g., female, couple, and zip code‚Äìlevel demographic information). This rich set of variables supports both a direct replication of the original study‚Äôs effects as well as further investigation into heterogeneous treatment responses.\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWe perform two analyses: T-test Approach and Linear Regression Approach.\n\nT-test Approach\n\n\nCode\nimport scipy.stats as stats\n\n# Drop missing values for the variable mrm2 (months since last donation)\ndata_n = data.dropna(subset=[\"mrm2\"])\n\ntreatment_group = data_n[data_n[\"treatment\"] == 1][\"mrm2\"]\ncontrol_group   = data_n[data_n[\"treatment\"] == 0][\"mrm2\"]\n\nt_stat, p_val = stats.ttest_ind(treatment_group, control_group, equal_var=True)\n\nprint(\"T-test for mrm2:\")\nprint(\"  t-statistic =\", round(t_stat, 4))\nprint(\"  p-value     =\", round(p_val, 4))\n\n\nT-test for mrm2:\n  t-statistic = 0.1195\n  p-value     = 0.9049\n\n\nWe first conduct a two-sample t-test comparing the means of mrm2 between the treatment and control groups using the formula:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_0^2}{n_0}}}\n\\]\nSince the p-value is much greater than 0.05, we fail to reject the null hypothesis. There is no statistically significant difference between the two groups in terms of months since last donation.\n\n\nLinear Regression Approach\n\n\nCode\nimport statsmodels.formula.api as smf\n\n# Run a linear regression of mrm2 on the treatment indicator.\n# The model: mrm2 = beta0 + beta1 * treatment + error.\nmodel = smf.ols(\"mrm2 ~ treatment\", data=data).fit()\n\nprint(\"Linear Regression Output (mrm2 ~ treatment):\")\nprint(model.summary().tables[1])\n\n\nLinear Regression Output (mrm2 ~ treatment):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\n\nWe then estimate the following regression model:\n\\[\n\\text{mrm2}_i = \\beta_0 + \\beta_1 \\cdot \\text{treatment}_i + \\epsilon_i\n\\]\nThe estimated coefficient for treatment is:\n\ncoef: 0.0137\nstd err: 0.115\nt-stat: 0.119\np-value: 0.905\n95% CI: ([-0.211, 0.238])\n\nThis result matches the t-test output exactly (as expected, since the two methods are equivalent in this context). The coefficient is close to zero and not statistically significant, providing further evidence that the treatment and control groups are balanced on this variable.\nThe purpose of these tests is to assess whether the randomization successfully created equivalent groups on observable characteristics. Since both the t-test and regression show no significant differences in mrm2, we conclude that this variable is balanced across the groups. This kind of table and analysis appears in Table 1 of the original Karlan and List (2007) paper. Including such balance checks is standard in experimental work‚Äîit helps ensure that differences in outcomes can be attributed to the treatment and not to confounding factors."
  },
  {
    "objectID": "projects/project1/hw1_blog.html#experimental-results",
    "href": "projects/project1/hw1_blog.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, we analyze whether matched donations lead to an increased response rate‚Äîthat is, an increased probability that someone donates. In our data, the binary variable gave equals 1 if a donation was made and 0 otherwise.\nBelow is a barplot with two bars: one for the control group and one for the treatment group. Each bar represents the proportion of people in that group who donated.\n\n\nCode\n# Make sure to drop any missing values in the outcome variable (if needed)\ndata_n = data.dropna(subset=[\"gave\"])\n\n# Compute the proportion of individuals who gave (gave==1) by treatment group.\nprop_data = data_n.groupby(\"treatment\")[\"gave\"].mean().reset_index()\nprop_data[\"Group\"] = prop_data[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\n# Create the barplot using seaborn.\nplt.figure(figsize=(6, 4))\nsns.barplot(x=\"Group\", y=\"gave\", data=prop_data, palette=\"viridis\")\nplt.ylabel(\"Proportion Donated\")\nplt.title(\"Donation Response Rate by Group\")\nplt.ylim(0, 0.3)\nplt.show()\n\n\nC:\\Users\\22344\\AppData\\Local\\Temp\\ipykernel_1044\\256720481.py:10: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nT-test for Donation Response\n\n\nCode\n# Drop rows with missing values in 'gave' (if any)\ndata_n = data.dropna(subset=[\"gave\"])\n\n# Separate the binary outcome for the treatment and control groups.\ntreatment_outcome = data_n[data_n[\"treatment\"] == 1][\"gave\"]\ncontrol_outcome   = data_n[data_n[\"treatment\"] == 0][\"gave\"]\n\n# Conduct an independent-samples t-test assuming equal variances.\nt_stat, p_val = stats.ttest_ind(treatment_outcome, control_outcome, equal_var=True)\nprint(\"T-test for the binary outcome 'gave':\")\nprint(\"  t-statistic =\", round(t_stat, 4))\nprint(\"  p-value     =\", round(p_val, 4))\n\n\nT-test for the binary outcome 'gave':\n  t-statistic = 3.1014\n  p-value     = 0.0019\n\n\nBivariate Linear Regression\n\n\nCode\nmodel_lm = smf.ols(\"gave ~ treatment\", data=data).fit()\nprint(\"Linear Regression Results (dependent variable: gave):\")\nprint(model_lm.summary().tables[1])\n\n\nLinear Regression Results (dependent variable: gave):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\n\n\nThe t-test result shows a t-statistic of 3.10 and a p-value of 0.0019, which is well below the common threshold of 0.05. This means the observed difference in donation rates is very unlikely to be due to chance. The regression confirms this: the estimated treatment effect (treatment coefficient) is 0.0042, or about a 0.42 percentage point increase in the likelihood of donating for those who received the matching offer. Although this might sound small, it‚Äôs a relative increase of over 23% compared to the control group‚Äôs base rate of ~1.8% (the intercept).\nThese findings are a powerful demonstration of how framing and perceived impact can affect real-world decisions. Even though the monetary benefit to the donor doesn‚Äôt change, simply highlighting that their donation will be matched makes giving feel more impactful or worthwhile. This reflects a broader pattern in behavioral economics: people are more motivated when they believe their actions make a bigger difference People aren‚Äôt just giving because they can‚Äîthey‚Äôre giving because the matching offer increases the perceived value of their contribution. This supports the theoretical idea that lowering the ‚Äúprice‚Äù of a public good (or increasing its ‚Äúvalue‚Äù) increases its provision, even in voluntary settings.\nTo assess the effect of being offered a matched donation on the likelihood of donating, we estimate a probit regression where the outcome is a binary variable (gave) indicating whether a donation was made. The key independent variable is the treatment assignment (treatment = 1 if the individual received a letter with a matching grant, 0 otherwise).\n\n\nCode\ndata_n = data.dropna(subset=[\"gave\"])\n\n# Run the probit regression: donate (gave) ~ treatment.\nmodel_probit = smf.probit(\"gave ~ treatment\", data=data_n).fit()\nprint(\"Probit Regression Results (dependent variable: gave):\")\nprint(model_probit.summary())\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\nProbit Regression Results (dependent variable: gave):\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Thu, 24 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        00:04:00   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nThis finding replicates Table 3, Column 1 of Karlan and List (2007), reinforcing the conclusion that even modest changes to how charitable appeals are framed‚Äîsuch as offering to match a donor‚Äôs contribution‚Äîcan have a measurable impact on donor behavior. These results support the broader theory that people are more likely to contribute when they perceive that their donation will go further. From a fundraising strategy perspective, this suggests that matching offers are an effective lever to boost response rates.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nIn this section, we assess whether the size of the matched donation (i.e., the match ratio) has an effect on whether people donate. In our experiment, treatment letters were randomly assigned to include one of three match ratios: 1:1, 2:1, or 3:1. According to the original study, the figures suggest that while a matching offer increases the likelihood of donating relative to a control group, further increases in the match ratio (from 1:1 to 2:1 and from 2:1 to 3:1) do not have additional effects.\n\nT-test by Match Ratio We first restrict our analysis to the treatment group, then calculate the donation rate (mean of gave) for each match ratio, and finally perform t-tests.\n\n\n\nCode\ntreatment_data = data[data[\"treatment\"] == 1].dropna(subset=[\"gave\"])\n\n# Assuming the dataset contains three dummy variables:\n# 'ratio' indicates a 1:1 match, 'ratio2' indicates a 2:1 match,\n# and 'ratio3' indicates a 3:1 match.\n# (In each observation, exactly one of these is equal to 1.)\n\n# Subset donation outcomes for each match ratio.\ngroup_ratio1 = treatment_data[treatment_data[\"ratio\"] == 1][\"gave\"]\ngroup_ratio2 = treatment_data[treatment_data[\"ratio2\"] == 1][\"gave\"]\ngroup_ratio3 = treatment_data[treatment_data[\"ratio3\"] == 1][\"gave\"]\n\n# T-test: Compare 1:1 vs. 2:1\nt_stat_1_2, p_val_1_2 = stats.ttest_ind(group_ratio1, group_ratio2, equal_var=True)\nprint(\"T-test for 1:1 vs 2:1 match rates:\")\nprint(\"  t-statistic =\", round(t_stat_1_2, 4))\nprint(\"  p-value     =\", round(p_val_1_2, 4))\n\n# T-test: Compare 2:1 vs. 3:1\nt_stat_2_3, p_val_2_3 = stats.ttest_ind(group_ratio2, group_ratio3, equal_var=True)\nprint(\"\\nT-test for 2:1 vs 3:1 match rates:\")\nprint(\"  t-statistic =\", round(t_stat_2_3, 4))\nprint(\"  p-value     =\", round(p_val_2_3, 4))\n\n\nT-test for 1:1 vs 2:1 match rates:\n  t-statistic = -0.965\n  p-value     = 0.3345\n\nT-test for 2:1 vs 3:1 match rates:\n  t-statistic = -0.0501\n  p-value     = 0.96\n\n\n\nRegression Analysis by Match Ratio We now create a variable ratio1 (which is identical to ratio) for clarity, and then regress gave on the match ratio dummies without an intercept.\n\n\n\nCode\ntreatment_data = treatment_data.copy()\ntreatment_data[\"ratio1\"] = treatment_data[\"ratio\"]\n\n# Run the regression without an intercept to estimate group means directly.\nmodel_match = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=treatment_data).fit()\nprint(\"Regression Results (no intercept):\")\nprint(model_match.summary().tables[1])\n\n\nRegression Results (no intercept):\n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nratio1[Control]          0          0        nan        nan           0           0\nratio1[1]           0.0207      0.001     14.912      0.000       0.018       0.023\nratio1[2]       -4.365e+09   3.12e+10     -0.140      0.889   -6.55e+10    5.67e+10\nratio1[3]       -1.274e+10    9.1e+10     -0.140      0.889   -1.91e+11    1.66e+11\nratio2           4.365e+09   3.12e+10      0.140      0.889   -5.67e+10    6.55e+10\nratio3           1.274e+10    9.1e+10      0.140      0.889   -1.66e+11    1.91e+11\n===================================================================================\n\n\nC:\\Users\\22344\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\regression\\linear_model.py:1966: RuntimeWarning:\n\ndivide by zero encountered in scalar divide\n\n\n\n\nCalculating and Comparing Differences\n\n\n\nCode\ntreatment_data = treatment_data.copy()\ntreatment_data[\"ratio1\"] = pd.to_numeric(treatment_data[\"ratio\"], errors='coerce')\ntreatment_data[\"ratio2\"] = pd.to_numeric(treatment_data[\"ratio2\"], errors='coerce')\ntreatment_data[\"ratio3\"] = pd.to_numeric(treatment_data[\"ratio3\"], errors='coerce')\n\n# Run the regression without an intercept:\nmodel_match = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=treatment_data).fit()\n\n# Check what parameter names are in the fitted model:\nprint(\"Model Parameters:\", model_match.params.index.tolist())\n\n# Now extract the coefficients:\ncoeffs = model_match.params\nmu_ratio1 = coeffs[\"ratio1\"]\nmu_ratio2 = coeffs[\"ratio2\"]\nmu_ratio3 = coeffs[\"ratio3\"]\n\ndiff_2_vs_1_coef = mu_ratio2 - mu_ratio1\ndiff_3_vs_2_coef = mu_ratio3 - mu_ratio2\n\nprint(\"Fitted Coefficient Differences:\")\nprint(\"  Difference between 2:1 and 1:1 (regression):\", round(diff_2_vs_1_coef, 4))\nprint(\"  Difference between 3:1 and 2:1 (regression):\", round(diff_3_vs_2_coef, 4))\n\n\nModel Parameters: ['ratio1', 'ratio2', 'ratio3']\nFitted Coefficient Differences:\n  Difference between 2:1 and 1:1 (regression): -0.0396\n  Difference between 3:1 and 2:1 (regression): -0.0206\n\n\nBoth our t‚Äëtests and regression analyses converge on the same conclusion: although the introduction of a matching donation offer (versus no match) increases the probability that an individual donates, increasing the match ratio beyond 1:1 (i.e., comparing 1:1 to 2:1 and 2:1 to 3:1) does not further increase the response rate in a statistically significant way. In plain language, donors seem to respond to the existence of a matching offer rather than the exact size of the match. This finding supports the observation mentioned by the authors on page 8 of the Karlan and List paper that ‚Äúfigures suggest‚Äù no additional effect of increasing the match ratio beyond 1:1.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nFull Sample Analysis: We run a t‚Äëtest and bivariate linear regression of the donation amount (variable amount) on treatment status, using all observations. Since many people did not donate (i.e.¬†their donation amount is zero), this analysis mixes two effects:\n\n\nThe increased likelihood of donating, and\nThe actual amount given among donors.\n\n\n\nCode\ndata_n = data.dropna(subset=[\"amount\"])\n\n# T-test comparing donation amounts between treatment and control groups\ntreatment_amount = data_n[data_n[\"treatment\"] == 1][\"amount\"]\ncontrol_amount   = data_n[data_n[\"treatment\"] == 0][\"amount\"]\n\nt_stat_full, p_val_full = stats.ttest_ind(treatment_amount, control_amount, equal_var=True)\nprint(\"Full Sample T-test for donation amount:\")\nprint(\"  t-statistic =\", round(t_stat_full, 4))\nprint(\"  p-value =\", round(p_val_full, 4))\n\n# Bivariate linear regression using the full sample.\nmodel_full = smf.ols(\"amount ~ treatment\", data=data_n).fit()\nprint(\"\\nFull Sample Linear Regression Results (amount ~ treatment):\")\nprint(model_full.summary().tables[1])\n\n\nFull Sample T-test for donation amount:\n  t-statistic = 1.8605\n  p-value = 0.0628\n\nFull Sample Linear Regression Results (amount ~ treatment):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\n\n\nThe t‚Äëtest produced a t‚Äëstatistic of approximately 1.861 with a p‚Äëvalue of 0.0628. This result is marginally non‚Äësignificant by the conventional 0.05 threshold, suggesting that‚Äîwhen considering the entire sample‚Äîthe average donation in the treatment group is somewhat higher than in the control group, but the evidence is not strong enough to conclusively rule out that this difference might be due to chance. The estimated intercept was 0.8133, and the treatment coefficient was 0.1536 with a corresponding t‚Äëstatistic of 1.861 and a p‚Äëvalue of 0.063. These results are consistent with the t‚Äëtest.\nThere is an increase in the overall donation amount when a matching donation offer is included. However, much of this overall effect is likely driven by an increase in the probability of donating rather than by a substantially higher donation amount among those who do give. That is, while the matching offer appears to encourage more people to donate, it does not necessarily lead existing donors to give much more money.\n\n\nCode\ndata_n = data.dropna(subset=[\"amount\"])\n\ndonors = data[data[\"amount\"] &gt; 0]\n\n# Conduct a t-test comparing donation amounts among donors \n# between treatment and control groups.\ntreatment_amount_cond = donors[donors[\"treatment\"] == 1][\"amount\"]\ncontrol_amount_cond   = donors[donors[\"treatment\"] == 0][\"amount\"]\n\nt_stat_cond, p_val_cond = stats.ttest_ind(treatment_amount_cond, control_amount_cond, equal_var=True)\nprint(\"Conditional T-test for donation amount (donors only):\")\nprint(\"  t-statistic =\", round(t_stat_cond, 4))\nprint(\"  p-value =\", round(p_val_cond, 4))\n\n# Run a bivariate linear regression of donation amount on treatment among donors.\nmodel_cond = smf.ols(\"amount ~ treatment\", data=donors).fit()\nprint(\"\\nConditional Linear Regression Results (amount ~ treatment) among donors:\")\nprint(model_cond.summary().tables[1])\n\n\nConditional T-test for donation amount (donors only):\n  t-statistic = -0.5808\n  p-value = 0.5615\n\nConditional Linear Regression Results (amount ~ treatment) among donors:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\n\n\nThe t-test indicates that the average donation amount among donors does not differ significantly between the treatment and control groups. The intercept is estimated at 45.54 representing the average donation amount among control group donors. The treatment coefficient is estimated at -1.67 with a standard error of 2.87 (t = -0.581, p = 0.561). This coefficient indicates that, among donors, those in the treatment group donate on average about $1.67 less than those in the control group. However, the difference is not statistically significant.\nWhen we restrict the analysis to individuals who donate, the matching treatment does not appear to significantly affect how much they donate. Although the unconditional analysis (which includes both donors and non-donors) shows that the treatment increases the overall probability of donating, the conditional analysis suggests that, among those who choose to donate, the donation size remains largely unchanged. In summary, these results suggest that the matching offer primarily works by encouraging more people to donate; it does not have a significant effect on increasing the average donation size among those who already decide to give.\n\n\nCode\ndata_n = data.dropna(subset=[\"amount\"])\n\n# Restrict analysis to people who donated (amount &gt; 0)\ndonors = data_n[data[\"amount\"] &gt; 0]\n\n# Separate the donation amounts by treatment group\ntreatment_donors = donors[donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors   = donors[donors[\"treatment\"] == 0][\"amount\"]\n\n# Calculate the mean donation amount for each group\nmean_treatment = treatment_donors.mean()\nmean_control   = control_donors.mean()\n\n# Set up the plotting area\nplt.figure(figsize=(12, 5))\n\n# Plot histogram for the Control group\nplt.subplot(1, 2, 1)\nsns.histplot(control_donors, bins=30, kde=False, color=\"lightblue\")\nplt.axvline(mean_control, color=\"red\", linestyle=\"dashed\", linewidth=2)\nplt.title(\"Control Group: Donation Amounts (Donors Only)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.text(mean_control, plt.ylim()[1]*0.8, f\"Mean = {mean_control:.2f}\", color=\"red\", ha=\"center\")\n\n# Plot histogram for the Treatment group\nplt.subplot(1, 2, 2)\nsns.histplot(treatment_donors, bins=30, kde=False, color=\"lightgreen\")\nplt.axvline(mean_treatment, color=\"red\", linestyle=\"dashed\", linewidth=2)\nplt.title(\"Treatment Group: Donation Amounts (Donors Only)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.text(mean_treatment, plt.ylim()[1]*0.8, f\"Mean = {mean_treatment:.2f}\", color=\"red\", ha=\"center\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/project1/hw1_blog.html#simulation-experiment",
    "href": "projects/project1/hw1_blog.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo demonstrate the Law of Large Numbers, we simulate a large number of individual draws from two Bernoulli distributions: Control group: p=0.018 Treatment group: p=0.022\n\n\nCode\nimport numpy as np\n\nnp.random.seed(1234)\n\n# Parameters for Bernoulli distributions\np_control = 0.018\np_treatment = 0.022\n\n# Simulate 100,000 draws from the control distribution\ncontrol_full = np.random.binomial(1, p_control, size=100000)\n\n# Randomly select 10,000 draws from the control draws\ncontrol_sample = np.random.choice(control_full, size=10000, replace=False)\n\n# Simulate 10,000 draws from the treatment distribution\ntreatment_sample = np.random.binomial(1, p_treatment, size=10000)\n\n# Calculate the vector of differences (treatment - control) for each pair\ndifferences = treatment_sample - control_sample\n\n# Compute the cumulative average of these differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, 10000 + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(10,6))\nplt.plot(cumulative_avg, label=\"Cumulative Average of Differences\")\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label=\"True Difference (0.004)\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.title(\"Law of Large Numbers: Convergence of the Cumulative Average\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nBelow, we produce histograms of the sample means for both groups and overlay the appropriate Normal density.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define parameters for the Bernoulli distributions\np_control = 0.018\np_treatment = 0.022\nreps = 1000  # number of experiments (repetitions)\nsample_sizes = [50, 200, 500, 1000]\n\n# Dictionary to store the average difference for each sample size\ndiff_results = {}\n\nfor n in sample_sizes:\n    diffs = []\n    for i in range(reps):\n        # Draw n observations for control and treatment groups\n        control_sample = np.random.binomial(1, p_control, size=n)\n        treatment_sample = np.random.binomial(1, p_treatment, size=n)\n        diff = np.mean(treatment_sample) - np.mean(control_sample)\n        diffs.append(diff)\n    diff_results[n] = np.array(diffs)\n\n# Plot 4 histograms in a 2x2 grid\nfig, axs = plt.subplots(2, 2, figsize=(12, 10))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    sns.histplot(diff_results[n], bins=30, stat=\"density\", color=\"lightblue\", ax=axs[i])\n    # Overlay a vertical dashed red line at the true difference (0.004)\n    axs[i].axvline(p_treatment - p_control, color=\"red\", linestyle=\"--\", linewidth=2, label=\"True Difference (0.004)\")\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Average Difference (Treatment - Control)\")\n    axs[i].set_ylabel(\"Density\")\n    axs[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Average Differences\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxing Liu",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nYuxing Liu\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYuxing Liu\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project2/hw2_questions.html",
    "href": "projects/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Load the datasets\nairbnb_df = pd.read_csv('airbnb.csv')\nblueprinty_df = pd.read_csv('blueprinty.csv')\n\n\n# Plot histogram of patents by customer status\nplt.figure(figsize=(10, 6))\nsns.histplot(data=blueprinty_df, x='patents', hue='iscustomer', multiple='stack', kde=True, bins=15)\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.legend(title='Customer Status', labels=['Non-Customer', 'Customer'])\nplt.show()\n\n# Calculate and compare means of patents by customer status\npatents_means = blueprinty_df.groupby('iscustomer')['patents'].mean()\nprint(\"Mean Number of Patents by Customer Status:\")\nprint(patents_means)\n\n\n\n\n\n\n\n\nMean Number of Patents by Customer Status:\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nThe histogram shows that customers (blue) tend to have slightly more patents than non-customers (orange).\nNon-customers have a higher concentration of individuals with fewer patents (around 0-3 patents), while customers have a more spread-out distribution with a concentration around 3-5 patents.The blue distribution (for customers) is more spread out, whereas the orange distribution (for non-customers) is more concentrated around lower patent counts.\nThe mean number of patents for non-customers is approximately 3.47. The mean number of patents for customers is approximately 4.13. Customers tend to have a higher number of patents on average compared to non-customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Compare regions by customer status\nregion_counts = blueprinty_df.groupby('iscustomer')['region'].value_counts().unstack(fill_value=0)\nprint(\"Region Distribution by Customer Status:\")\nprint(region_counts)\n\n# Compare ages by customer status\nage_stats = blueprinty_df.groupby('iscustomer')['age'].describe()\nprint(\"\\nAge Statistics by Customer Status:\")\nprint(age_stats)\n\nRegion Distribution by Customer Status:\nregion      Midwest  Northeast  Northwest  South  Southwest\niscustomer                                                 \n0               187        273        158    156        245\n1                37        328         29     35         52\n\nAge Statistics by Customer Status:\n             count       mean       std   min   25%   50%    75%   max\niscustomer                                                            \n0           1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\n1            481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\n\nNon-customers are distributed across regions, with the highest counts in the Northeast (273) and Southwest (245). They are less represented in the Northeast and South compared to customers. Customers are predominantly concentrated in the Northeast (328), but they are less represented in the Northwest (29) and South (35) regions. This suggests that customers are more likely to be from the Northeast, while non-customers are spread across regions in a more even manner.\nThe average age for non-customers is 26.1 years, while the average age for customers is slightly higher at 26.9 years. The standard deviation for both groups is around 6.95 for non-customers and 7.81 for customers, indicating similar variability in age across the two groups. Both groups have a similar age range, with non-customers having a minimum age of 9 years and a maximum of 47.5 years, and customers ranging from 10 years to 49 years. The age difference between customers and non-customers is slight, though customers tend to be slightly older on average.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function (PMF) for a Poisson distribution is: \\(f(ùëå | Œª) = (Œª^ùëå * e^{-Œª}) / Y!\\)\nWhere: ùëå is the observed count of events (e.g., the number of patents), Œª is the rate parameter (expected number of events), ùëå! is the factorial of ùëåÔºå e^(-Œª) is the Poisson probability of zero events.\nGiven a set of independent observations ùëå1Ôºåùëå2, ‚Ä¶, ùëån, the likelihood fucntion is: \\(L(Œª) = ‚àè[i=1 to n] (Œª^ùëå_i * e^{-Œª}) / ùëå_i!\\)\nThe log-likelihood function is: \\(log L(Œª) = Œ£[i=1 to n] (Y_i * log(Œª) - Œª - log(Y_i!))\\)\n\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Parameters:\n    lambda_ : float\n        The rate parameter (expected number of events)\n    Y : array-like\n        The observed data (number of events)\n    \"\"\"\n    # Poisson log-likelihood formula: sum(Y_i * log(lambda) - lambda - log(Y_i!))\n    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - np.log(factorial(Y)))\n    return log_likelihood\n\n\nfrom scipy.special import factorial \nY = blueprinty_df['patents'].values\n\n# Range of lambdas for plotting\nlambda_range = np.linspace(0.1, 10, 100)\n\n# Compute log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_range]\n\n# Plotting the log-likelihood vs. lambda\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_range, log_likelihoods, label=\"Log-Likelihood\")\nplt.title(\"Log-Likelihood vs. Lambda for Poisson Model\")\nplt.xlabel(\"Lambda (Rate Parameter)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe log-likelihood function for a Poisson distribution is: \\(log L(Œª) = Œ£[i=1 to n] (Y_i * log(Œª) - Œª - log(Y_i!))\\)\nWhere: Y_i are the observed counts (number of events), Œª is the rate parameter (mean number of events), ùëõ is the number of observations.\nWe take the derivative of log L(Œª) with respect to Œª to find the maximum likelihood estimate. The derivative is: \\(d/dŒª log L(Œª) = Œ£[i=1 to n] (Y_i / Œª - 1)\\) This derivative tells us how the log-likelihood changes with respect to Œª.\nTo find the value of Œª that maximizes the log-likelihood, we set the derivative equal to zero and simplify. \\((1 / Œª) * Œ£[i=1 to n] Y_i = n\\)\nNow, solving for Œª: \\(Œª = (1 / n) * Œ£[i=1 to n] Y_i\\) This is the sample mean of Y, denoted as Y_bar.\nThe Maximum Likelihood Estimator (MLE) for Œª is the sample mean Y_bar, which is intuitive because for a Poisson distribution, the mean is Œª. Thus: \\(Œª_MLE = Y_bar\\)\n\ndef poisson_neg_loglikelihood(lambda_, Y):\n    # Log-likelihood formula: sum(Y_i * log(lambda) - lambda - log(Y_i!))\n    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - np.log(factorial(Y)))\n    return -log_likelihood  # We return the negative for minimization\n\n# Load the dataset (assuming blueprinty_df is available)\n# blueprinty_df = pd.read_csv('path_to_data.csv')  # Uncomment if needed\nY = blueprinty_df['patents'].values  # Example: Observed number of patents\n\n# Use scipy's minimize to find the MLE of lambda\nresult = minimize(poisson_neg_loglikelihood, x0=1, args=(Y,), bounds=[(0.001, None)])\n\n# Get the estimated lambda (MLE)\nlambda_mle = result.x[0]\nprint(f\"MLE for lambda: {lambda_mle}\")\n\nMLE for lambda: 3.6846667021660804\n\n\nThe Maximum Likelihood Estimator (MLE) for ùúÜ, based on the observed data, is approximately 3.685. This means that the best estimate for the expected number of patents (events) per unit of time is 3.685, according to the Poisson model. Since the mean of a Poisson distribution is equal to ùúÜ, this result suggests that, on average, there are about 3.685 patents awarded over the given period for each observed entity.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_neg_loglikelihood(beta, Y, X):\n    Y = np.round(Y).astype(int)\n    \n    # Calculate the linear predictor X_i' beta\n    linear_predictor = np.dot(X, beta)\n    \n    # Calculate the log-likelihood for Poisson regression\n    log_likelihood = np.sum(Y * linear_predictor - np.exp(linear_predictor) - np.log(factorial(Y)))\n    \n    # Return the negative log-likelihood for minimization\n    return -log_likelihood\n\ntodo: Use your function along with R‚Äôs optim() or Python‚Äôs sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1‚Äôs to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R‚Äôs glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty‚Äôs software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Load the datasets\nairbnb_df = pd.read_csv('airbnb.csv')\nblueprinty_df = pd.read_csv('blueprinty.csv')\n\n\n# Plot histogram of patents by customer status\nplt.figure(figsize=(10, 6))\nsns.histplot(data=blueprinty_df, x='patents', hue='iscustomer', multiple='stack', kde=True, bins=15)\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.legend(title='Customer Status', labels=['Non-Customer', 'Customer'])\nplt.show()\n\n# Calculate and compare means of patents by customer status\npatents_means = blueprinty_df.groupby('iscustomer')['patents'].mean()\nprint(\"Mean Number of Patents by Customer Status:\")\nprint(patents_means)\n\n\n\n\n\n\n\n\nMean Number of Patents by Customer Status:\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nThe histogram shows that customers (blue) tend to have slightly more patents than non-customers (orange).\nNon-customers have a higher concentration of individuals with fewer patents (around 0-3 patents), while customers have a more spread-out distribution with a concentration around 3-5 patents.The blue distribution (for customers) is more spread out, whereas the orange distribution (for non-customers) is more concentrated around lower patent counts.\nThe mean number of patents for non-customers is approximately 3.47. The mean number of patents for customers is approximately 4.13. Customers tend to have a higher number of patents on average compared to non-customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Compare regions by customer status\nregion_counts = blueprinty_df.groupby('iscustomer')['region'].value_counts().unstack(fill_value=0)\nprint(\"Region Distribution by Customer Status:\")\nprint(region_counts)\n\n# Compare ages by customer status\nage_stats = blueprinty_df.groupby('iscustomer')['age'].describe()\nprint(\"\\nAge Statistics by Customer Status:\")\nprint(age_stats)\n\nRegion Distribution by Customer Status:\nregion      Midwest  Northeast  Northwest  South  Southwest\niscustomer                                                 \n0               187        273        158    156        245\n1                37        328         29     35         52\n\nAge Statistics by Customer Status:\n             count       mean       std   min   25%   50%    75%   max\niscustomer                                                            \n0           1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\n1            481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\n\nNon-customers are distributed across regions, with the highest counts in the Northeast (273) and Southwest (245). They are less represented in the Northeast and South compared to customers. Customers are predominantly concentrated in the Northeast (328), but they are less represented in the Northwest (29) and South (35) regions. This suggests that customers are more likely to be from the Northeast, while non-customers are spread across regions in a more even manner.\nThe average age for non-customers is 26.1 years, while the average age for customers is slightly higher at 26.9 years. The standard deviation for both groups is around 6.95 for non-customers and 7.81 for customers, indicating similar variability in age across the two groups. Both groups have a similar age range, with non-customers having a minimum age of 9 years and a maximum of 47.5 years, and customers ranging from 10 years to 49 years. The age difference between customers and non-customers is slight, though customers tend to be slightly older on average.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function (PMF) for a Poisson distribution is: \\(f(ùëå | Œª) = (Œª^ùëå * e^{-Œª}) / Y!\\)\nWhere: ùëå is the observed count of events (e.g., the number of patents), Œª is the rate parameter (expected number of events), ùëå! is the factorial of ùëåÔºå e^(-Œª) is the Poisson probability of zero events.\nGiven a set of independent observations ùëå1Ôºåùëå2, ‚Ä¶, ùëån, the likelihood fucntion is: \\(L(Œª) = ‚àè[i=1 to n] (Œª^ùëå_i * e^{-Œª}) / ùëå_i!\\)\nThe log-likelihood function is: \\(log L(Œª) = Œ£[i=1 to n] (Y_i * log(Œª) - Œª - log(Y_i!))\\)\n\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Parameters:\n    lambda_ : float\n        The rate parameter (expected number of events)\n    Y : array-like\n        The observed data (number of events)\n    \"\"\"\n    # Poisson log-likelihood formula: sum(Y_i * log(lambda) - lambda - log(Y_i!))\n    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - np.log(factorial(Y)))\n    return log_likelihood\n\n\nfrom scipy.special import factorial \nY = blueprinty_df['patents'].values\n\n# Range of lambdas for plotting\nlambda_range = np.linspace(0.1, 10, 100)\n\n# Compute log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_range]\n\n# Plotting the log-likelihood vs. lambda\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_range, log_likelihoods, label=\"Log-Likelihood\")\nplt.title(\"Log-Likelihood vs. Lambda for Poisson Model\")\nplt.xlabel(\"Lambda (Rate Parameter)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe log-likelihood function for a Poisson distribution is: \\(log L(Œª) = Œ£[i=1 to n] (Y_i * log(Œª) - Œª - log(Y_i!))\\)\nWhere: Y_i are the observed counts (number of events), Œª is the rate parameter (mean number of events), ùëõ is the number of observations.\nWe take the derivative of log L(Œª) with respect to Œª to find the maximum likelihood estimate. The derivative is: \\(d/dŒª log L(Œª) = Œ£[i=1 to n] (Y_i / Œª - 1)\\) This derivative tells us how the log-likelihood changes with respect to Œª.\nTo find the value of Œª that maximizes the log-likelihood, we set the derivative equal to zero and simplify. \\((1 / Œª) * Œ£[i=1 to n] Y_i = n\\)\nNow, solving for Œª: \\(Œª = (1 / n) * Œ£[i=1 to n] Y_i\\) This is the sample mean of Y, denoted as Y_bar.\nThe Maximum Likelihood Estimator (MLE) for Œª is the sample mean Y_bar, which is intuitive because for a Poisson distribution, the mean is Œª. Thus: \\(Œª_MLE = Y_bar\\)\n\ndef poisson_neg_loglikelihood(lambda_, Y):\n    # Log-likelihood formula: sum(Y_i * log(lambda) - lambda - log(Y_i!))\n    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - np.log(factorial(Y)))\n    return -log_likelihood  # We return the negative for minimization\n\n# Load the dataset (assuming blueprinty_df is available)\n# blueprinty_df = pd.read_csv('path_to_data.csv')  # Uncomment if needed\nY = blueprinty_df['patents'].values  # Example: Observed number of patents\n\n# Use scipy's minimize to find the MLE of lambda\nresult = minimize(poisson_neg_loglikelihood, x0=1, args=(Y,), bounds=[(0.001, None)])\n\n# Get the estimated lambda (MLE)\nlambda_mle = result.x[0]\nprint(f\"MLE for lambda: {lambda_mle}\")\n\nMLE for lambda: 3.6846667021660804\n\n\nThe Maximum Likelihood Estimator (MLE) for ùúÜ, based on the observed data, is approximately 3.685. This means that the best estimate for the expected number of patents (events) per unit of time is 3.685, according to the Poisson model. Since the mean of a Poisson distribution is equal to ùúÜ, this result suggests that, on average, there are about 3.685 patents awarded over the given period for each observed entity.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_neg_loglikelihood(beta, Y, X):\n    Y = np.round(Y).astype(int)\n    \n    # Calculate the linear predictor X_i' beta\n    linear_predictor = np.dot(X, beta)\n    \n    # Calculate the log-likelihood for Poisson regression\n    log_likelihood = np.sum(Y * linear_predictor - np.exp(linear_predictor) - np.log(factorial(Y)))\n    \n    # Return the negative log-likelihood for minimization\n    return -log_likelihood\n\ntodo: Use your function along with R‚Äôs optim() or Python‚Äôs sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1‚Äôs to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R‚Äôs glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty‚Äôs software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#airbnb-case-study",
    "href": "projects/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nairbnb_df.head()\n\nmissing_data = airbnb_df.isnull().sum()\n\n# Drop rows with missing values in critical columns\nclean_airbnb_df = airbnb_df.dropna(subset=['number_of_reviews', 'price', 'bathrooms', 'bedrooms'])\n\n# Handle missing review scores: If needed, we can either drop or impute missing values in these columns\nclean_airbnb_df = clean_airbnb_df.dropna(subset=['review_scores_cleanliness', 'review_scores_location', 'review_scores_value'])\n\n# Ensure that columns are the correct data types (e.g., numeric columns like price, reviews, bathrooms, bedrooms)\nclean_airbnb_df['price'] = pd.to_numeric(clean_airbnb_df['price'], errors='coerce')\nclean_airbnb_df['number_of_reviews'] = pd.to_numeric(clean_airbnb_df['number_of_reviews'], errors='coerce')\nclean_airbnb_df['bathrooms'] = pd.to_numeric(clean_airbnb_df['bathrooms'], errors='coerce')\nclean_airbnb_df['bedrooms'] = pd.to_numeric(clean_airbnb_df['bedrooms'], errors='coerce')\n\n# Create dummy variables for 'room_type' (drop the first category to avoid collinearity)\nclean_airbnb_df = pd.get_dummies(clean_airbnb_df, columns=['room_type'], drop_first=True)\n\n# Check the cleaned dataset\nprint(clean_airbnb_df.head())\n\n   Unnamed: 0    id  days last_scraped host_since  bathrooms  bedrooms  price  \\\n0           1  2515  3130     4/2/2017   9/6/2008        1.0       1.0     59   \n1           2  2595  3127     4/2/2017   9/9/2008        1.0       0.0    230   \n3           4  3831  3038     4/2/2017  12/7/2008        1.0       1.0     89   \n5           6  5099  2981     4/2/2017   2/2/2009        1.0       1.0    212   \n6           7  5107  2981     4/2/2017   2/2/2009        1.0       2.0    250   \n\n   number_of_reviews  review_scores_cleanliness  review_scores_location  \\\n0                150                        9.0                     9.0   \n1                 20                        9.0                    10.0   \n3                116                        9.0                     9.0   \n5                 60                        9.0                     9.0   \n6                 60                       10.0                     9.0   \n\n   review_scores_value instant_bookable  room_type_Private room  \\\n0                  9.0                f                    True   \n1                  9.0                f                   False   \n3                  9.0                f                   False   \n5                  9.0                f                   False   \n6                 10.0                f                   False   \n\n   room_type_Shared room  \n0                  False  \n1                  False  \n3                  False  \n5                  False  \n6                  False  \n\n\nExploratory Data Analysis(EDA):\n\nsummary_stats = clean_airbnb_df.describe()\n\n# Plot the distribution of key variables\nplt.figure(figsize=(12, 8))\n\n# Distribution of number of reviews (proxy for bookings)\nplt.subplot(2, 2, 1)\nsns.histplot(clean_airbnb_df['number_of_reviews'], kde=True)\nplt.title('Distribution of Number of Reviews')\n\n# Distribution of price\nplt.subplot(2, 2, 2)\nsns.histplot(clean_airbnb_df['price'], kde=True)\nplt.title('Distribution of Price')\n\n# Distribution of number of bedrooms\nplt.subplot(2, 2, 3)\nsns.histplot(clean_airbnb_df['bedrooms'], kde=True)\nplt.title('Distribution of Bedrooms')\n\n# Distribution of bathrooms\nplt.subplot(2, 2, 4)\nsns.histplot(clean_airbnb_df['bathrooms'], kde=True)\nplt.title('Distribution of Bathrooms')\n\nplt.tight_layout()\nplt.show()\n\n# Correlation matrix to check for relationships\ncorr_matrix = clean_airbnb_df[['number_of_reviews', 'price', 'bathrooms', 'bedrooms', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value']].corr()\n\n# Plot the heatmap of correlations\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix of Key Variables')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPossion Regression Model:\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  }
]