---
title: "Poisson Regression Examples"
author: "Yuxing Liu"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

```{python}
import pandas as pd
import numpy as np
from scipy.optimize import minimize
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

# Load the datasets
airbnb_df = pd.read_csv('airbnb.csv')
blueprinty_df = pd.read_csv('blueprinty.csv')
```

```{python}
# Plot histogram of patents by customer status
plt.figure(figsize=(10, 6))
sns.histplot(data=blueprinty_df, x='patents', hue='iscustomer', multiple='stack', kde=True, bins=15)
plt.title('Histogram of Number of Patents by Customer Status')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.legend(title='Customer Status', labels=['Non-Customer', 'Customer'])
plt.show()

# Calculate and compare means of patents by customer status
patents_means = blueprinty_df.groupby('iscustomer')['patents'].mean()
print("Mean Number of Patents by Customer Status:")
print(patents_means)
```

The histogram shows that customers (blue) tend to have slightly more patents than non-customers (orange).

Non-customers have a higher concentration of individuals with fewer patents (around 0-3 patents), while customers have a more spread-out distribution with a concentration around 3-5 patents.The blue distribution (for customers) is more spread out, whereas the orange distribution (for non-customers) is more concentrated around lower patent counts.

The mean number of patents for non-customers is approximately 3.47. The mean number of patents for customers is approximately 4.13. Customers tend to have a higher number of patents on average compared to non-customers.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
# Compare regions by customer status
region_counts = blueprinty_df.groupby('iscustomer')['region'].value_counts().unstack(fill_value=0)
print("Region Distribution by Customer Status:")
print(region_counts)

# Compare ages by customer status
age_stats = blueprinty_df.groupby('iscustomer')['age'].describe()
print("\nAge Statistics by Customer Status:")
print(age_stats)
```

Non-customers are distributed across regions, with the highest counts in the Northeast (273) and Southwest (245). They are less represented in the Northeast and South compared to customers. Customers are predominantly concentrated in the Northeast (328), but they are less represented in the Northwest (29) and South (35) regions. This suggests that customers are more likely to be from the Northeast, while non-customers are spread across regions in a more even manner.

The average age for non-customers is 26.1 years, while the average age for customers is slightly higher at 26.9 years. The standard deviation for both groups is around 6.95 for non-customers and 7.81 for customers, indicating similar variability in age across the two groups. Both groups have a similar age range, with non-customers having a minimum age of 9 years and a maximum of 47.5 years, and customers ranging from 10 years to 49 years. The age difference between customers and non-customers is slight, though customers tend to be slightly older on average.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

The probability mass function (PMF) for a Poisson distribution is:
$f(ùëå | Œª) = (Œª^ùëå * e^{-Œª}) / Y!$

Where:
ùëå is the observed count of events (e.g., the number of patents),
Œª is the rate parameter (expected number of events),
ùëå! is the factorial of ùëåÔºå
e^(-Œª) is the Poisson probability of zero events.

Given a set of independent observations ùëå1Ôºåùëå2, ..., ùëån, the likelihood fucntion is:
$L(Œª) = ‚àè[i=1 to n] (Œª^ùëå_i * e^{-Œª}) / ùëå_i!$

The log-likelihood function is:
$log L(Œª) = Œ£[i=1 to n] (Y_i * log(Œª) - Œª - log(Y_i!))$

```{python}
def poisson_loglikelihood(lambda_, Y):
    """
    Parameters:
    lambda_ : float
        The rate parameter (expected number of events)
    Y : array-like
        The observed data (number of events)
    """
    # Poisson log-likelihood formula: sum(Y_i * log(lambda) - lambda - log(Y_i!))
    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - np.log(factorial(Y)))
    return log_likelihood
```

```{python}
from scipy.special import factorial 
Y = blueprinty_df['patents'].values

# Range of lambdas for plotting
lambda_range = np.linspace(0.1, 10, 100)

# Compute log-likelihood for each lambda
log_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_range]

# Plotting the log-likelihood vs. lambda
plt.figure(figsize=(10, 6))
plt.plot(lambda_range, log_likelihoods, label="Log-Likelihood")
plt.title("Log-Likelihood vs. Lambda for Poisson Model")
plt.xlabel("Lambda (Rate Parameter)")
plt.ylabel("Log-Likelihood")
plt.grid(True)
plt.legend()
plt.show()
```

The log-likelihood function for a Poisson distribution is:
$log L(Œª) = Œ£[i=1 to n] (Y_i * log(Œª) - Œª - log(Y_i!))$

Where:
Y_i are the observed counts (number of events),
Œª is the rate parameter (mean number of events),
ùëõ is the number of observations.

We take the derivative of log L(Œª) with respect to Œª to find the maximum likelihood estimate. The derivative is:
$d/dŒª log L(Œª) = Œ£[i=1 to n] (Y_i / Œª - 1)$
This derivative tells us how the log-likelihood changes with respect to Œª.

To find the value of Œª that maximizes the log-likelihood, we set the derivative equal to zero and simplify.
$(1 / Œª) * Œ£[i=1 to n] Y_i = n$

Now, solving for Œª:
$Œª = (1 / n) * Œ£[i=1 to n] Y_i$
This is the sample mean of Y, denoted as Y_bar.

The Maximum Likelihood Estimator (MLE) for Œª is the sample mean Y_bar, which is intuitive because for a Poisson distribution, the mean is Œª. Thus:
$Œª_MLE = Y_bar$

```{python}
def poisson_neg_loglikelihood(lambda_, Y):
    # Log-likelihood formula: sum(Y_i * log(lambda) - lambda - log(Y_i!))
    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - np.log(factorial(Y)))
    return -log_likelihood  # We return the negative for minimization

# Load the dataset (assuming blueprinty_df is available)
# blueprinty_df = pd.read_csv('path_to_data.csv')  # Uncomment if needed
Y = blueprinty_df['patents'].values  # Example: Observed number of patents

# Use scipy's minimize to find the MLE of lambda
result = minimize(poisson_neg_loglikelihood, x0=1, args=(Y,), bounds=[(0.001, None)])

# Get the estimated lambda (MLE)
lambda_mle = result.x[0]
print(f"MLE for lambda: {lambda_mle}")
```

The Maximum Likelihood Estimator (MLE) for ùúÜ, based on the observed data, is approximately 3.685. This means that the best estimate for the expected number of patents (events) per unit of time is 3.685, according to the Poisson model. Since the mean of a Poisson distribution is equal to ùúÜ, this result suggests that, on average, there are about 3.685 patents awarded over the given period for each observed entity.

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

```{python}
def poisson_regression_neg_loglikelihood(beta, Y, X):
    linear_predictor = np.dot(X, beta)
    log_likelihood = np.sum(Y * linear_predictor - np.exp(linear_predictor) - np.log(factorial(Y)))
    return -log_likelihood
```

_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._

```{python}
# initial_beta = np.zeros(X.shape[1])

# # Use scipy.optimize.minimize to find the MLE (negative log-likelihood)
# result = minimize(poisson_regression_neg_loglikelihood, initial_beta, args=(Y, X), method='BFGS')

# # Extract estimated coefficients (MLE) and Hessian matrix
# beta_hat = result.x
# hessian_matrix = result.hess_inv

# # Compute standard errors from the diagonal of the inverse Hessian
# standard_errors = np.sqrt(np.diag(hessian_matrix))

# # Display the results: coefficients and standard errors
# coefficients_and_errors = pd.DataFrame({
#     'Coefficient': beta_hat,
#     'Standard Error': standard_errors
# }, index=X.columns)

# # Display the table with coefficients and standard errors
# tools.display_dataframe_to_user(name="Coefficients and Standard Errors", dataframe=coefficients_and_errors)

# # Check the results using GLM from statsmodels
# poisson_model = sm.GLM(Y, X, family=sm.families.Poisson())
# poisson_results = poisson_model.fit()

# # Print the summary of GLM results
# print(poisson_results.summary())
```

_todo: Check your results using R's glm() function or Python sm.GLM() function._

_todo: Interpret the results._ 

_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::

```{python}
airbnb_df.head()

missing_data = airbnb_df.isnull().sum()

# Drop rows with missing values in critical columns
clean_airbnb_df = airbnb_df.dropna(subset=['number_of_reviews', 'price', 'bathrooms', 'bedrooms'])

# Handle missing review scores: If needed, we can either drop or impute missing values in these columns
clean_airbnb_df = clean_airbnb_df.dropna(subset=['review_scores_cleanliness', 'review_scores_location', 'review_scores_value'])

# Ensure that columns are the correct data types (e.g., numeric columns like price, reviews, bathrooms, bedrooms)
clean_airbnb_df['price'] = pd.to_numeric(clean_airbnb_df['price'], errors='coerce')
clean_airbnb_df['number_of_reviews'] = pd.to_numeric(clean_airbnb_df['number_of_reviews'], errors='coerce')
clean_airbnb_df['bathrooms'] = pd.to_numeric(clean_airbnb_df['bathrooms'], errors='coerce')
clean_airbnb_df['bedrooms'] = pd.to_numeric(clean_airbnb_df['bedrooms'], errors='coerce')

# Create dummy variables for 'room_type' (drop the first category to avoid collinearity)
clean_airbnb_df = pd.get_dummies(clean_airbnb_df, columns=['room_type'], drop_first=True)

# Check the cleaned dataset
print(clean_airbnb_df.head())
```

Exploratory Data Analysis(EDA):
```{python}
summary_stats = clean_airbnb_df.describe()

# Plot the distribution of key variables
plt.figure(figsize=(12, 8))

# Distribution of number of reviews (proxy for bookings)
plt.subplot(2, 2, 1)
sns.histplot(clean_airbnb_df['number_of_reviews'], kde=True)
plt.title('Distribution of Number of Reviews')

# Distribution of price
plt.subplot(2, 2, 2)
sns.histplot(clean_airbnb_df['price'], kde=True)
plt.title('Distribution of Price')

# Distribution of number of bedrooms
plt.subplot(2, 2, 3)
sns.histplot(clean_airbnb_df['bedrooms'], kde=True)
plt.title('Distribution of Bedrooms')

# Distribution of bathrooms
plt.subplot(2, 2, 4)
sns.histplot(clean_airbnb_df['bathrooms'], kde=True)
plt.title('Distribution of Bathrooms')

plt.tight_layout()
plt.show()

# Correlation matrix to check for relationships
corr_matrix = clean_airbnb_df[['number_of_reviews', 'price', 'bathrooms', 'bedrooms', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value']].corr()

# Plot the heatmap of correlations
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Key Variables')
plt.show()
```

Possion Regression Model:
```{python}
clean_airbnb_df = clean_airbnb_df.rename(columns={'room_type_Shared room':'room_type_Shared_room'})
clean_airbnb_df = clean_airbnb_df.rename(columns={'room_type_Private room':'room_type_Private_room'})
# Define the formula for Poisson regression
formula = 'number_of_reviews ~ price + bedrooms + bathrooms + review_scores_cleanliness + review_scores_location + review_scores_value + room_type_Private_room + room_type_Shared_room'

# Fit the Poisson regression model using GLM (Generalized Linear Model)
poisson_model = sm.GLM.from_formula(formula, data=clean_airbnb_df, family=sm.families.Poisson(), link=sm.families.links.log()).fit()

# Show the results of the regression
print(poisson_model.summary())
```

The intercept of 3.7136 represents the log of the expected number of reviews for a baseline listing (i.e., a listing with price=0, bedrooms=0, bathrooms=0, and average review scores). This is a reference point, but not directly interpretable in a meaningful way for real listings. The coefficient for room_type_Private_room[T.True] is 0.0074. This indicates that, holding other variables constant, a Private room listing is associated with a slight increase in the log of the expected number of reviews, compared to a listing that is not a Private room (the reference category). Exponentiating this coefficient gives a rate ratio of exp(0.0074) ‚âà 1.0074, suggesting a 0.74% higher rate of reviews for Private room listings compared to the reference category.

Overall summary, Private room has a slightly positive effect on reviews, while Shared room has a significantly negative effect on reviews. There is a very small negative effect of price on the number of reviews, suggesting that higher prices are weakly associated with fewer reviews. Higher cleanliness scores are associated with more reviews, while higher location and value scores are associated with fewer reviews.

```{python}
# Get exponentiated coefficients and standard errors (rate ratios)
rate_ratios = np.exp(poisson_model.params)
rate_ratio_se = np.exp(poisson_model.bse)

# Create a DataFrame to display the rate ratios and their standard errors
rate_ratios_df = pd.DataFrame({
    'Rate Ratio': rate_ratios,
    'Standard Error': rate_ratio_se
})

print(rate_ratios_df)
```

The baseline rate of the outcome is very high when all predictors are at reference levels. 
Private rooms: No significant impact on the outcome rate (rate ratio close to 1).
Shared rooms: Associated with a lower outcome rate (rate ratio < 1).
Price has a minimal effect on the outcome rate (rate ratio close to 1).
Review Scores:
Cleanliness: Higher cleanliness scores increase the outcome rate.
Location: Higher location scores slightly decrease the outcome rate.
Value: Higher value scores slightly decrease the outcome rate.





